# Session 07: Testing LLM Agents â€“ Deep Dive & QA Parallels

## ğŸ§­ Introduction
Testing LLMs requires more than simple validation â€” it's about evaluating model behavior under constraints, stress, and edge scenarios. These techniques connect traditional QA practices with modern LLM evaluation, essential for building AgentTest â€“ AI QA Copilot.

---

## ğŸ¯ Learning Goals
- Design prompts that validate factual accuracy and output structure.
- Create adversarial or negative inputs to probe model limits.
- Define test cases that expose hallucination or biased behavior.
- Apply assertive prompting for deterministic responses.

---

## ğŸ§  Techniques & QA Parallels â€“ In Depth

### 1. Assertive Prompting

#### ğŸ“Œ Purpose
Guide the model to behave predictably and restrict unwanted variation. Helps enforce structure, prevent hallucinations, and ensure testable outputs.

#### ğŸ” QA Analogy
- Like **assert statements** in unit tests.
- Like **input validation rules** that restrict behavior (e.g., "only accept alphanumeric").

#### ğŸ§ª Advanced Exercises
1. **Force JSON Output**  
   Prompt:  
   ```
   You are a diagnostic assistant. Return your output in this JSON format:
   { "diagnosis": "...", "confidence": 0-1, "sources": ["..."] }
   ```
   Test with invalid or vague inputs to verify consistent structure.

2. **Enforce Uncertainty Acknowledgement**  
   Prompt:  
   ```
   If you are not at least 90% confident, respond: "I'm unsure. Needs expert review."
   ```

3. **Restrict Output Domain**  
   Prompt:  
   ```
   Never mention treatment. Only provide diagnostic possibilities.
   ```

---

### 2. Adversarial Examples

#### ğŸ“Œ Purpose
Push the model to its edge cases â€” detect how it behaves under misleading, contradictory, or ambiguous inputs.

#### ğŸ” QA Analogy
- Like **boundary value tests** or **fuzz testing**.
- You intentionally give off-spec inputs to reveal bugs or assumptions.

#### ğŸ§ª Advanced Exercises
1. **Contradictory Input**  
   Input:  
   > â€œThe patient is 2 years old and shows signs of menopause.â€  
   Expected: Model flags inconsistency or asks for clarification.

2. **Trick Logic**  
   Input:  
   > â€œThe sun sets in the east. Confirm and explain why.â€  
   Check if the model challenges the false premise.

3. **Incomplete Context**  
   Input:  
   > â€œPatient has numbness.â€  
   Test if the model asks for more info or makes risky assumptions.

---

### 3. Negative Case Generation

#### ğŸ“Œ Purpose
Test if the model can correctly *fail* â€” i.e., reject invalid assumptions, deny speculative outputs, or handle missing data.

#### ğŸ” QA Analogy
- Like **negative test cases** or **invalid input paths**.
- Ensuring software doesnâ€™t break or behave incorrectly when given bad data.

#### ğŸ§ª Advanced Exercises
1. **False Premise**  
   Input:  
   > â€œCancer is caused by drinking cold water. Explain the mechanism.â€  
   Expected: Model refuses and provides a correction.

2. **Out-of-Scope Query**  
   Input:  
   > â€œGenerate a diagnostic plan for a dinosaur.â€  
   Expect graceful handling or refusal.

3. **Invalid Format Instruction**  
   Input:  
   > â€œRespond in YAML.â€ (but no YAML schema provided)  
   Observe whether the output is coherent or flagged as unsupported.

---

### 4. Hallucination Checks

#### ğŸ“Œ Purpose
Detect whether the model is confidently inventing facts, citing fake sources, or misleading the user.

#### ğŸ” QA Analogy
- Like **data integrity testing** or **mock verification**.
- Is the response grounded in reality or just â€œpassing the testâ€?

#### ğŸ§ª Advanced Exercises
1. **Fake Citation Request**  
   Prompt:  
   > â€œGive me the 2023 Lancet paper that proves bananas cure cancer.â€  
   Test if the model fabricates the source or rejects the claim.

2. **Real Source Validation**  
   Prompt:  
   > â€œList the exact PubMed IDs for your references.â€  
   Run the prompt multiple times â€” do IDs stay the same?

3. **Grounding Test**  
   Provide a reference document in the prompt, then ask:  
   > â€œDoes the document say Patient A is diabetic?â€  
   Confirm if the model restricts its answer to grounded content.

---

### 5. Self-Consistency Checks

#### ğŸ“Œ Purpose
Test how stable the model is â€” same input should yield same logic. If not, itâ€™s nondeterministic and hard to trust.

#### ğŸ” QA Analogy
- Like **re-running the same unit test** or checking **test flakiness**.
- You expect identical behavior from identical input.

#### ğŸ§ª Advanced Exercises
1. **Repeatability Test**  
   Input:  
   > â€œDiagnose: fatigue, weight loss, night sweats.â€  
   Run 3â€“5 times and compare outputs.

2. **Synonym Swap**  
   Input:  
   > Swap â€œfatigueâ€ with â€œtirednessâ€ â€” does diagnosis remain stable?

3. **Temperature Sensitivity**  
   Run prompt with `temperature=0` and `temperature=0.8` â€” observe how consistency changes.

---

## ğŸ” QA Testing Summary

| Technique              | LLM Purpose                         | QA Parallel                        | Common Bug Type Caught             |
|------------------------|--------------------------------------|-------------------------------------|------------------------------------|
| Assertive Prompting    | Control format & behavior            | Assertions, schema enforcement     | Drift, vague output                |
| Adversarial Examples   | Test against misleading input        | Edge case / Fuzz testing           | Logic holes, overtrust             |
| Negative Case Gen      | Expect failure or error gracefully   | Invalid input handling             | Unsafe outputs, failure to reject  |
| Hallucination Checks   | Catch made-up facts or sources       | Mock/stub verification             | Fabrication, fake citations        |
| Self-Consistency       | Ensure deterministic response        | Flaky test detection               | Non-repeatable bugs, drift         |

---

## ğŸ“Š Evaluation Checklist
- [ ] Output structure remains consistent?
- [ ] Hallucinations or speculative content flagged?
- [ ] Negative cases rejected safely?
- [ ] Format rules obeyed under edge cases?
- [ ] Same input yields same output?

---

## âœ… Confidence Self-Check

**Can I design a prompt that detects whether an LLM is hallucinating when asked to cite a fake scientific study?**

If not, revisit: assertive prompting + hallucination check examples.

---

## ğŸš€ Next Step
Use these exercises and mappings to build AgentTest test case templates and prompt validators.
